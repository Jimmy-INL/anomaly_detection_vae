{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "rZdgv1WDN7VF",
    "outputId": "f8c88ea4-0324-4948-8228-66cbb1a0787a",
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install timesynth\n",
    "# TIMESYNTH TO GENERATE TIME SERIESES\n",
    "import ts_syntheticData\n",
    "from ts_syntheticData import insert_anomalies\n",
    "# CUSTOM DEFINED DATALOADERS AND MODELS\n",
    "from ts_dataloader import get_datasets, load_data\n",
    "from ts_training import train_network\n",
    "from ts_VI_LSTM import Variational_LSTM, loss_normal2d, loss_normal2d_lognormal\n",
    "from ts_simple_LSTM_net import Standard_LSTM, loss_function_normal\n",
    "from ts_anomaly_function import detect_anomalies, detect_anomalies_VAE\n",
    "# PYTORCH\n",
    "import torch\n",
    "from torch import optim, distributions\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# MATPLOTLIB NUMPY\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "colab_type": "code",
    "id": "bf-7zCZgN7VR",
    "outputId": "f4c3f080-e760-4731-b79e-da5ac05e201a",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "USEDATASET = 0  # simple synthetic dataset\n",
    "USEDATASET = 1 # correlated synthetic dataset\n",
    "USEDATASET = 2 # realistic weather dataset\n",
    "\n",
    "# anomaly parameters\n",
    "add_anomalies = False\n",
    "if (USEDATASET == 0) or (USEDATASET == 1):\n",
    "    add_anomalies = True\n",
    "# anomaly probability\n",
    "anomalies_p = 0.001\n",
    "# anomaly magnitude\n",
    "anomalies_m = 3\n",
    "\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "\n",
    "# number of samples in the time series\n",
    "train_T = 10000\n",
    "valid_T = 5000\n",
    "test_T = 5000\n",
    "# width of the window of each time series\n",
    "W = 10000\n",
    "# check for inconsistency\n",
    "if W > train_T:\n",
    "    raise ValueError(\"The time window cannot be longer than the training data sequence available.\")\n",
    "\n",
    "T = train_T + valid_T + test_T\n",
    "# the size of train and validation, test is the remainder of the data\n",
    "t_v_t_split = [train_T/T,valid_T/T]\n",
    "\n",
    "#load the data in, this is supposed to happen only once\n",
    "raw_signals = load_data(USEDATASET, T)\n",
    "\n",
    "if add_anomalies:\n",
    "    timeseries_signals, timeseries_labels = insert_anomalies(raw_signals, magnitude=anomalies_m, p=anomalies_p)\n",
    "else:\n",
    "    timeseries_signals = raw_signals\n",
    "    timeseries_labels = None\n",
    "\n",
    "features, train_dataset, valid_dataset, test_dataset = get_datasets(USEDATASET, t_v_t_split, W, device, timeseries_signals, timeseries_labels)\n",
    "\n",
    "train_timeseries_signals = train_dataset.get_data()[0]\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title(\"Normalized signals used in training\")\n",
    "plt.plot(list(range(len(train_timeseries_signals))), train_timeseries_signals)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6_bfdG_N7VY",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Here we can change to batch size\n",
    "# batch size (number of serieses of length W)\n",
    "B = 1\n",
    "# check for consistency, we don't want to pad the sequence if possible\n",
    "if (train_T - W + 1) % B != 0 :\n",
    "    raise ValueError(\"The batch size chosen will result in different sized batches during training\")\n",
    "\n",
    "# create the Synthetic Dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=False)\n",
    "#valid_loader = DataLoader(valid_dataset, shuffle=False)\n",
    "#seq_loader = DataLoader(valid_dataset, shuffle=False)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZ42a3ojJj-F"
   },
   "outputs": [],
   "source": [
    "# function used to plot the anomalies in the dataset\n",
    "def plot_anomalies(anomaly_data, sequence, A=0, B=1000):\n",
    "\n",
    "    labels = anomaly_data[\"outlier_label\"]\n",
    "    if sequence.shape[0] != len(labels):\n",
    "        raise ValueError(\"The length of the sequence and the number of labels whould be the same.\")\n",
    "    \n",
    "    Ti = max(0, A)\n",
    "    Tf = min(B, sequence.shape[0])\n",
    "    time_axis = list(range(Ti,Tf-1))\n",
    "    \n",
    "    figures= []\n",
    "    for dimension in range(sequence.shape[1]):\n",
    "\n",
    "        fig = plt.figure(figsize = (8,4))\n",
    "        plt.title(\"Feature {}\".format(dimension))\n",
    "        for t in range(Ti, Tf-1):\n",
    "            plt.plot(t, sequence[t, dimension], 'o', color=\"red\" if labels[t] else \"blue\", markersize = 6 if labels[t] else 2)\n",
    "        plt.plot(time_axis, sequence[Ti:Tf-1,dimension], label = \"Real data\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.legend()\n",
    "        figures.append(fig)\n",
    "        plt.show()\n",
    "    \n",
    "    return figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "q_flE9cAN7Vl",
    "outputId": "4e92319d-68e8-4593-97e4-62fb772732e0",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs_VAE = 200\n",
    "params_distribution = 2\n",
    "hidden_dim_gen = 128\n",
    "hidden_dim_rec = 128\n",
    "latent_dim = 2\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#define the network\n",
    "net = Variational_LSTM(features, params_distribution, hidden_dim_rec, hidden_dim_gen, latent_dim)\n",
    "#push it to the correct device\n",
    "net.to(device)\n",
    "#define the loss function\n",
    "loss_function = loss_normal2d\n",
    "#define the optimizer with the learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay = 1e-4)\n",
    "#here we define the scheduler\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# beta annealing to try to address the vanishing KL\n",
    "def constant_schedule(beta, epoch):\n",
    "    return 0\n",
    "def monotonic_sigmoid_schedule(beta, epoch):\n",
    "    k = 1.4 # steepness\n",
    "    m = 40 # half time\n",
    "    return (k**(epoch-m))/((k**(epoch-m))+1)\n",
    "def monotonic_linear_schedule(beta, epoch):\n",
    "    start_epoch = 0\n",
    "    len_annealing = 100\n",
    "    if epoch < start_epoch:\n",
    "        return 0\n",
    "    elif epoch < start_epoch + len_annealing:\n",
    "        return min(beta + 1./len_annealing,1.)\n",
    "    else:\n",
    "        return 1\n",
    "def monotonic_weighted_linear_schedule(beta, epoch):\n",
    "    start_epoch = 50\n",
    "    len_annealing = 50\n",
    "    maximum_beta = 0.0001\n",
    "    if epoch < start_epoch:\n",
    "        return 0\n",
    "    elif epoch < start_epoch + len_annealing:\n",
    "        return min(beta + maximum_beta/len_annealing,maximum_beta)\n",
    "    else:\n",
    "        return maximum_beta\n",
    "def cyclic_annealing_schedule(beta, epoch):\n",
    "    M = 4\n",
    "    L = epochs_VAE / M\n",
    "    R = 0.5\n",
    "    tau = (epoch % math.ceil(L))/L\n",
    "    f_type = \"linear\" #linear, sigmoid\n",
    "    if tau <= R:\n",
    "        return increasing_function(tau, f_type, R)\n",
    "    else:\n",
    "        return 1\n",
    "def increasing_function(tau, f_type, R):\n",
    "    if f_type == \"linear\":\n",
    "        return tau/R\n",
    "    elif f_type == \"sigmoid\":\n",
    "        k = 1.04 # steepness\n",
    "        m = R/2 # half time\n",
    "        return (k**(1000*(tau-m)))/((k**(1000*((tau-m))))+1)\n",
    "\n",
    "chosen_annealing = cyclic_annealing_schedule\n",
    "\n",
    "betas = []\n",
    "beta = 0\n",
    "for epoch in range(epochs_VAE):\n",
    "    betas.append(chosen_annealing(beta, epoch))\n",
    "    beta = betas[-1]\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title(\"Annealing of the KL term\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"beta\")\n",
    "plt.plot(list(range(len(betas))), betas)\n",
    "\n",
    "\n",
    "#run the training\n",
    "net_state_dict, train_fig = train_network(device, train_loader, valid_dataset, epochs_VAE, net, \\\n",
    "                                          loss_function, optimizer, chosen_annealing, scheduler = scheduler, \\\n",
    "                                          p_anomaly = 1e-5, plotting = False)\n",
    "net_state_dict_VAE = net_state_dict\n",
    "train_fig_VAE = train_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs_VAE = 200\n",
    "# beta annealing to try to address the vanishing KL\n",
    "def constant_schedule(beta, epoch):\n",
    "    return 0\n",
    "def monotonic_sigmoid_schedule(beta, epoch):\n",
    "    k = 1.4 # steepness\n",
    "    m = 40 # half time\n",
    "    return (k**(epoch-m))/((k**(epoch-m))+1)\n",
    "def monotonic_linear_schedule(beta, epoch):\n",
    "    start_epoch = 50\n",
    "    len_annealing = 100\n",
    "    if epoch < start_epoch:\n",
    "        return 0\n",
    "    elif epoch < start_epoch + len_annealing:\n",
    "        return min(beta + 1./len_annealing,1.)\n",
    "    else:\n",
    "        return 1\n",
    "def monotonic_weighted_linear_schedule(beta, epoch):\n",
    "    start_epoch = 50\n",
    "    len_annealing = 50\n",
    "    maximum_beta = 0.0001\n",
    "    if epoch < start_epoch:\n",
    "        return 0\n",
    "    elif epoch < start_epoch + len_annealing:\n",
    "        return min(beta + maximum_beta/len_annealing,maximum_beta)\n",
    "    else:\n",
    "        return maximum_beta\n",
    "def cyclic_annealing_schedule(beta, epoch):\n",
    "    M = 4\n",
    "    L = epochs_VAE / M\n",
    "    R = 0.5\n",
    "    tau = (epoch % math.ceil(L))/L\n",
    "    f_type = \"sigmoid\" #linear, sigmoid\n",
    "    if tau <= R:\n",
    "        return increasing_function(tau, f_type, R)\n",
    "    else:\n",
    "        return 1\n",
    "def increasing_function(tau, f_type, R):\n",
    "    if f_type == \"linear\":\n",
    "        return tau/R\n",
    "    elif f_type == \"sigmoid\":\n",
    "        k = 1.04 # steepness\n",
    "        m = R/2 # half time\n",
    "        return (k**(1000*(tau-m)))/((k**(1000*((tau-m))))+1)\n",
    "\n",
    "\n",
    "betas = []\n",
    "beta = 0\n",
    "for epoch in range(epochs_VAE):\n",
    "    betas.append(cyclic_annealing_schedule(beta, epoch))\n",
    "    beta = betas[-1]\n",
    "fig = plt.figure(figsize = (7,4.3))\n",
    "plt.title(\"Cyclic Schedule, linear increase, M = 4, R = 0.5\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"beta\")\n",
    "plt.plot(list(range(len(betas))), betas)\n",
    "fig.savefig(\"runs/vanishing_investigation/cycliclinearscheduleschedule405.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iQfciAngdTMp",
    "outputId": "af732a56-7ccf-46cc-898e-39e65a3edcfe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start time of plotting\n",
    "A = 0\n",
    "# end time of plotting\n",
    "B = 25000\n",
    "# select the sequence to test the network on\n",
    "sequence =  test_dataset.get_data()[0]\n",
    "sequence = train_dataset.get_data()[0]\n",
    "sequence = valid_dataset.get_data()[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # redefine the network\n",
    "    net = Variational_LSTM(features, params_distribution, hidden_dim_rec, hidden_dim_gen, latent_dim)\n",
    "    #push it to the correct device\n",
    "    net.to(device)\n",
    "    # load the state of the trained network\n",
    "    net.load_state_dict(net_state_dict_VAE)    \n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # get it to the device and  put the batch dimension\n",
    "    prepared_sequence = (sequence).to(device).unsqueeze(0)\n",
    "    \n",
    "    # run the model\n",
    "    output_model = net(prepared_sequence, device)\n",
    "\n",
    "    # extract the parameters at each time step\n",
    "    mu, logvar = torch.chunk(output_model[\"params\"], 2, dim=-1)\n",
    "    std_dev = torch.exp(logvar/2)\n",
    "    #rate = torch.exp(output_model[\"params\"])\n",
    "    #mu = 1/rate\n",
    "    #std_dev = mu\n",
    "    \n",
    "\n",
    "    # PLOTTING\n",
    "    # extract start and end points\n",
    "    Ti = max(0, A)\n",
    "    Tf = min(B, sequence.shape[0])\n",
    "    time_axis = list(range(Ti,Tf-1))\n",
    "    \n",
    "    output_model_figs_VAE = []\n",
    "    \n",
    "    for dimension in range(sequence.shape[1]):\n",
    "        mean = torch.mean(mu, dim = 1)[Ti:Tf-1,dimension].cpu()\n",
    "        std = 3*torch.mean(std_dev, dim = 1)[Ti:Tf-1,dimension].cpu()\n",
    "\n",
    "        fig = plt.figure(figsize = (8,4))\n",
    "        plt.title(\"Feature {}\".format(dimension))\n",
    "        plt.plot(time_axis, sequence[Ti+1:Tf,dimension], label = \"Real data\")\n",
    "        plt.plot(time_axis, mean, label=\"Mean\")\n",
    "        plt.fill_between(time_axis, mean - std, mean + std,\n",
    "                 color='blue', alpha=0.1, label=\"3 STD from mean\")\n",
    "        plt.legend()\n",
    "        output_model_figs_VAE.append(fig)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZXY2z5kLpOLy",
    "outputId": "56e35a62-a673-472a-804a-a96885df502e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# redefine the network\n",
    "net = Variational_LSTM(features, params_distribution, hidden_dim_rec, hidden_dim_gen, latent_dim)\n",
    "#push it to the correct device\n",
    "net.to(device)\n",
    "# load the state of the trained network\n",
    "net.load_state_dict(net_state_dict_VAE)\n",
    "\n",
    "sequence = valid_dataset.get_data()[0]\n",
    "#sequence = test_dataset.get_data()[0]\n",
    "\n",
    "percentage = 0.000001\n",
    "anomaly_data = detect_anomalies_VAE(sequence, net, device, percentage/100)\n",
    "anomalies_figs_VAE = plot_anomalies(anomaly_data, sequence, 0, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OFGEfTPrvkvZ",
    "outputId": "742e0c49-de3c-47f2-ffb4-5d449356e98d"
   },
   "outputs": [],
   "source": [
    "if not valid_dataset.has_labels():\n",
    "    accuracy_VAE = {\n",
    "        # number of labels correctly predicted\n",
    "        \"correct\": 0,  \n",
    "        # false positives (the datapoint was not an anomaly but it was predicted as one)\n",
    "        \"false_positives\": 0,\n",
    "        # false negatives (the datapoint was an anomaly abut it was not predicted as one)\n",
    "        \"false_negatives\": 0,\n",
    "        # total number of datapoints in the sequence\n",
    "        \"total\": 0,\n",
    "        # total number of anomalies\n",
    "        \"anomaly_count\": 0\n",
    "    }\n",
    "else:\n",
    "    predictions = anomaly_data[\"outlier_label\"]\n",
    "    ground_truth = valid_dataset.get_data()[1]\n",
    "    #ground_truth = test_dataset.get_data()[1]\n",
    "    correct = 0\n",
    "    total = len(predictions) - 1\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    anomaly_count = 0\n",
    "    for i in range(total):\n",
    "        if ground_truth[i] == True:\n",
    "            anomaly_count += 1\n",
    "        if predictions[i] == ground_truth[i]:\n",
    "            correct += 1\n",
    "        elif predictions[i] == False:\n",
    "            false_negatives += 1\n",
    "        elif predictions[i] == True:\n",
    "            false_positives += 1\n",
    "    accuracy_VAE = {\n",
    "        # number of labels correctly predicted\n",
    "        \"correct\": correct,  \n",
    "        # false positives (the datapoint was not an anomaly but it was predicted as one)\n",
    "        \"false_positives\": false_positives,\n",
    "        # false negatives (the datapoint was an anomaly abut it was not predicted as one)\n",
    "        \"false_negatives\": false_negatives,\n",
    "        # total number of datapoints in the sequence\n",
    "        \"total\": total,\n",
    "        # total number of anomalies\n",
    "        \"anomaly_count\": anomaly_count\n",
    "    }\n",
    "print(accuracy_VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4ykK0qFzN7Vf",
    "outputId": "d0ed2643-9c0e-48d5-d4a4-488c09964eb9",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs_LSTM = 200\n",
    "params_distribution = 2\n",
    "hidden_layer_size = 64\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#define the network\n",
    "net = Standard_LSTM(features, params_distribution, hidden_layer_size)\n",
    "#push it to the correct device\n",
    "net.to(device)\n",
    "#define the loss function\n",
    "loss_function = loss_function_normal\n",
    "#define the optimizer with the learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.05, weight_decay = 1e-3)\n",
    "#here we define the scheduler\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "#run the training\n",
    "net_state_dict, train_fig = train_network(device, train_loader, valid_dataset , epochs_LSTM, net, loss_function,\\\n",
    "                                          optimizer, scheduler = scheduler, plotting = False)\n",
    "net_state_dict_LSTM = net_state_dict\n",
    "train_fig_LSTM = train_fig\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Bmp-Z31wJj94",
    "outputId": "423f173a-83cd-4e4c-8911-29a3265ba58e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start time of plotting\n",
    "A = 0\n",
    "# end time of plotting\n",
    "B = 25000\n",
    "# select the sequence to test the network on\n",
    "sequence = test_dataset.get_data()[0]\n",
    "sequence = train_dataset.get_data()[0]\n",
    "sequence = valid_dataset.get_data()[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # redifine the network\n",
    "    net = Standard_LSTM(features, params_distribution, hidden_layer_size)\n",
    "    # push it to the correct device\n",
    "    net.to(device)\n",
    "    # load the state of the trained network\n",
    "    net.load_state_dict(net_state_dict_LSTM)   \n",
    "    \n",
    "    # get it to the device and  put the batch dimension\n",
    "    prepared_sequence = (sequence).to(device).unsqueeze(0)\n",
    "    \n",
    "    # run the model\n",
    "    output_model = net(prepared_sequence, device)\n",
    "\n",
    "    # extract the parameters at each time step\n",
    "    mu, logvar = torch.chunk(output_model[\"params\"], 2, dim=-1)\n",
    "    std_dev = torch.exp(logvar/2)\n",
    "\n",
    "    # PLOTTING\n",
    "    # extract start and end points\n",
    "    Ti = max(0, A)\n",
    "    Tf = min(B, sequence.shape[0])\n",
    "    time_axis = list(range(Ti,Tf-1))\n",
    "    \n",
    "    output_model_figs_LSTM = []\n",
    "    \n",
    "    for dimension in range(sequence.shape[1]):\n",
    "        mean = torch.mean(mu, dim = 1)[Ti:Tf-1,dimension].cpu()\n",
    "        std = 3*torch.mean(std_dev, dim = 1)[Ti:Tf-1,dimension].cpu()\n",
    "\n",
    "        fig = plt.figure(figsize = (8,4))\n",
    "        plt.title(\"Feature {}\".format(dimension))\n",
    "        plt.plot(time_axis, sequence[Ti+1:Tf,dimension], label = \"Real data\")\n",
    "        plt.plot(time_axis, mean, label=\"Mean\")\n",
    "        plt.fill_between(time_axis, mean - std, mean + std,\n",
    "                 color='blue', alpha=0.1, label=\"3 STD from mean\")\n",
    "        plt.legend()\n",
    "        output_model_figs_LSTM.append(fig)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bTBGbrofJj-K",
    "outputId": "d98f43d5-c51f-4092-c38e-b9240349ce8c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# redifine the network\n",
    "net = Standard_LSTM(features, params_distribution, hidden_layer_size)\n",
    "# push it to the correct device\n",
    "net.to(device)\n",
    "# load the state of the trained network\n",
    "net.load_state_dict(net_state_dict_LSTM)\n",
    "\n",
    "sequence = valid_dataset.get_data()[0]\n",
    "#sequence = test_dataset.get_data()[0]\n",
    "\n",
    "percentage = 0.5\n",
    "anomaly_data = detect_anomalies(sequence, net, device, percentage/100, False)\n",
    "anomalies_figs_LSTM = plot_anomalies(anomaly_data, sequence, 0, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_vKfjo7-Jj-S",
    "outputId": "3dae2499-28c4-4ddf-c13e-cdd15f2509ea"
   },
   "outputs": [],
   "source": [
    "if not valid_dataset.has_labels():\n",
    "    accuracy_LSTM = {\n",
    "        # number of labels correctly predicted\n",
    "        \"correct\": 0,  \n",
    "        # false positives (the datapoint was not an anomaly but it was predicted as one)\n",
    "        \"false_positives\": 0,\n",
    "        # false negatives (the datapoint was an anomaly abut it was not predicted as one)\n",
    "        \"false_negatives\": 0,\n",
    "        # total number of datapoints in the sequence\n",
    "        \"total\": 0,\n",
    "        # total number of anomalies\n",
    "        \"anomaly_count\": 0\n",
    "    }\n",
    "else:\n",
    "    predictions = anomaly_data[\"outlier_label\"]\n",
    "    ground_truth = valid_dataset.get_data()[1]\n",
    "    #ground_truth = test_dataset.get_data()[1]\n",
    "    correct = 0\n",
    "    total = len(predictions) - 1\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    anomaly_count = 0\n",
    "    for i in range(total):\n",
    "        if ground_truth[i] == True:\n",
    "            anomaly_count += 1\n",
    "        if predictions[i] == ground_truth[i]:\n",
    "            correct += 1\n",
    "        elif predictions[i] == False:\n",
    "            false_negatives += 1\n",
    "        elif predictions[i] == True:\n",
    "            false_positives += 1\n",
    "    accuracy_LSTM = {\n",
    "        # number of labels correctly predicted\n",
    "        \"correct\": correct,  \n",
    "        # false positives (the datapoint was not an anomaly but it was predicted as one)\n",
    "        \"false_positives\": false_positives,\n",
    "        # false negatives (the datapoint was an anomaly abut it was not predicted as one)\n",
    "        \"false_negatives\": false_negatives,\n",
    "        # total number of datapoints in the sequence\n",
    "        \"total\": total,\n",
    "        # total number of anomalies\n",
    "        \"anomaly_count\": anomaly_count\n",
    "    }\n",
    "print(accuracy_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lZc7nUGabhK"
   },
   "outputs": [],
   "source": [
    "def save_setup(run_name, \\\n",
    "               raw_signals, train_dataset, valid_dataset, test_dataset, net_state_dict_VAE, net_state_dict_LSTM, \\\n",
    "               anomalies_figures_LSTM, anomalies_figures_VAE, output_figures_LSTM, output_figures_VAE, \\\n",
    "               train_fig_LSTM, train_fig_VAE, \\\n",
    "               epochs_LSTM, hidden_layer_size, \\\n",
    "               epochs_VAE, params_distribution, hidden_dim_gen, hidden_dim_rec, latent_dim, \\\n",
    "               anomalies_m, anomalies_p, \\\n",
    "               accuracy_LSTM, accuracy_VAE):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "\n",
    "    now = datetime.now()\n",
    "\n",
    "    path = \"runs/\"+now.strftime(\"%Y%m%d_%H%M-\")+run_name\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "        print(OSError)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "\n",
    "    train_signals, train_labels = train_dataset.get_data()\n",
    "    valid_signals, valid_labels = valid_dataset.get_data()\n",
    "    test_signals,  test_labels  = test_dataset.get_data()\n",
    "    \n",
    "    np.save(path+\"/train_signals\", train_signals.numpy())\n",
    "    np.save(path+\"/valid_signals\", valid_signals.numpy())\n",
    "    np.save(path+\"/test_signals\", test_signals.numpy())\n",
    "    config_str = \"\"\n",
    "    if train_dataset.has_labels():\n",
    "        config_str += \"Labelled \\n\"\n",
    "        config_str += \"Anomalies magnitude: \"+str(anomalies_m)+\"\\n\"\n",
    "        config_str += \"Anomalies probability: \"+str(anomalies_p)+\"\\n\"\n",
    "        np.save(path+\"/train_labels\", train_labels.numpy())\n",
    "        np.save(path+\"/valid_labels\", valid_labels.numpy())\n",
    "        np.save(path+\"/test_labels\", test_labels.numpy())\n",
    "    else:\n",
    "        config_str += \"Not labelled \\n\"\n",
    "        config_str += \"Anomalies magnitude: 0\\n\"\n",
    "        config_str += \"Anomalies probability: 0\\n\"\n",
    "\n",
    "    train_fig_LSTM.savefig(path+\"/training_LSTM.pdf\")\n",
    "    train_fig_VAE.savefig(path+\"/training_VAE.pdf\")\n",
    "    \n",
    "    for i,figure in enumerate(anomalies_figures_LSTM):\n",
    "        figure.savefig(path+\"/results_anomaly_LSTM_\"+str(i)+\".pdf\")\n",
    "    for i,figure in enumerate(anomalies_figures_VAE):\n",
    "        figure.savefig(path+\"/results_anomaly_VAE_\"+str(i)+\".pdf\")\n",
    "    for i,figure in enumerate(output_figures_LSTM):\n",
    "        figure.savefig(path+\"/output_model_LSTM_\"+str(i)+\".pdf\")\n",
    "    for i,figure in enumerate(output_figures_VAE):\n",
    "        figure.savefig(path+\"/output_model_VAE_\"+str(i)+\".pdf\")\n",
    "    \n",
    "    config_str += \"Params out distrib: \"+ str(params_distribution) + \"\\n\"\n",
    "    config_str += \"LSTM\\n\"\n",
    "    config_str += \"Epochs: \"+str(epochs_LSTM)+\"\\n\"\n",
    "    config_str += \"Hidden_dim: \"+str(hidden_layer_size)+\"\\n\"\n",
    "    config_str += str(accuracy_LSTM)+\"\\n\"\n",
    "    config_str += \"VAE\\n\"\n",
    "    config_str += \"Epochs: \"+str(epochs_VAE)+\"\\n\"\n",
    "    config_str += \"Hidden dim gen: \"+str(hidden_dim_gen)+\"\\n\"\n",
    "    config_str += \"Hidden dim rec: \"+str(hidden_dim_rec)+\"\\n\"\n",
    "    config_str += \"Latent dim: \"+str(latent_dim)+\"\\n\"\n",
    "    config_str += str(accuracy_VAE)+\"\\n\"\n",
    "    \n",
    "    \n",
    "    with open(path+\"/configs.txt\", \"w\") as f:\n",
    "        f.write(config_str)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qh5g-HO3MNtb"
   },
   "outputs": [],
   "source": [
    "run_name = \"realdataset_10000_valid2\"\n",
    "\n",
    "save_setup(run_name, \\\n",
    "               raw_signals, train_dataset, valid_dataset, test_dataset, net_state_dict_VAE, net_state_dict_LSTM, \\\n",
    "               anomalies_figs_LSTM, anomalies_figs_VAE, output_model_figs_LSTM, output_model_figs_VAE, \\\n",
    "               train_fig_LSTM, train_fig_VAE, \\\n",
    "               epochs_LSTM, hidden_layer_size, \\\n",
    "               epochs_VAE, params_distribution, hidden_dim_gen, hidden_dim_rec, latent_dim, \\\n",
    "               anomalies_m, anomalies_p, \\\n",
    "               accuracy_LSTM, accuracy_VAE)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of network_tester.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
